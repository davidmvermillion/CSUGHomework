{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Load and Pre-Process Data",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "2f8397b8dcec4bb9b347002ea1a057d5",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "a580f65cd51448fb9481180c17adc763"
    },
    {
      "cell_type": "code",
      "source": "import tensorflow as tf\nimport seaborn as sns\nimport numpy as np\nfrom PIL import Image\nimport glob\nfrom collections import defaultdict\nfrom tensorflow import keras\nfrom tensorflow.keras import layers",
      "metadata": {
        "cell_id": "4dc0ec223e1e4c30b234a139240a659d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "4dc0ec223e1e4c30b234a139240a659d",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "IMG_SIZE = (94, 125)\n\ndef pixels_from_path(file_path):\n    im = Image.open(file_path)\n\n    im = im.resize(IMG_SIZE)\n    np_im = np.array(im)\n    # Matrix of pixel RGB values\n\n    return np_im\n\nshape_counts = defaultdict(int)\n\nfor i, cat in enumerate(glob.glob('cats/*')[:1000]):\n    if i%100 == 0:\n        print(i)\n    img_shape = pixels_from_path(cat).shape\n    shape_counts[str(img_shape)]= shape_counts[str(img_shape)]+ 1\n\nshape_items = list(shape_counts.items())\nshape_items.sort(key = lambda x: x[1])\nshape_items.reverse()\n\n# 10% of the data will automatically be used for validation\nvalidation_size = 0.1\nimg_size = IMG_SIZE # resize images to be 374x500 (most common shape)\nnum_channels = 3 # RGB\nsample_size = 8192 # We'll use 8192 pictures (2**13)\npixels_from_path(glob.glob('cats/*')[5]).shape\n\nSAMPLE_SIZE = 2048\n\nprint(\"loading training cat images...\")\ncat_train_set = np.asarray([pixels_from_path(cat) for cat in glob.glob('cats/*')[:SAMPLE_SIZE]])\n\nprint(\"loading training dog images...\")\ndog_train_set = np.asarray([pixels_from_path(dog) for dog in glob.glob('dogs/*')[:SAMPLE_SIZE]])\n\nvalid_size = 512\n\nprint(\"loading validation cat images...\")\ncat_valid_set = np.asarray([pixels_from_path(cat) for cat in glob.glob('cats/*')[-valid_size:]])\n\nprint(\"loading validation dog images...\")\ndog_valid_set = np.asarray([pixels_from_path(dog) for dog in glob.glob('dogs/*')[-valid_size:]])\n\nx_train = np.concatenate([cat_train_set, dog_train_set])\nlabels_train = np.asarray([1 for _ in range(SAMPLE_SIZE)]+[0 for _ in range(SAMPLE_SIZE)]) x_valid = np.concatenate([cat_valid_set, dog_valid_set])\nlabels_valid = np.asarray([1 for _ in range(valid_size)]+[0 for _ in range(valid_size)])",
      "metadata": {
        "cell_id": "96a2661ac5ee4e978ad96619855350d4",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "47269047f0224fb6ac29c1082f1b875b",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# CNN Implementation",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "656b8a484ac340c0aa35e260c1925d38",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "5adb8554369146fd8cb5126496f5483e"
    },
    {
      "cell_type": "code",
      "source": "total_pixels = img_size[0] * img_size[1] * 3\nfc_size = 512\ninputs = keras.Input(shape=(img_size[1], img_size[0], 3), name = 'ani_image')\nx = layers.Flatten(name = 'flattened_img')(inputs) # turn image to vector.\nx = layers.Dense(fc_size, activation = 'relu', name = 'first_layer')(x)\noutputs = layers.Dense(1, activation = 'sigmoid', name = 'class')(x)\n\nmodel = keras.Model(inputs = inputs, outputs = outputs)",
      "metadata": {
        "cell_id": "31e64201a6af44c18e77a1a976fecf7d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "bd3354114f204dea82f209bb930ee8f3",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3e480598-2102-4b3c-a7af-02aec7eef77c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "02805f786d7a4d39867d9a3c932e8033",
    "deepnote_execution_queue": []
  }
}