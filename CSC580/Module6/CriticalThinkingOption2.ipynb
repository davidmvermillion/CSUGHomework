{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Load and Pre-Process Data",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "2f8397b8dcec4bb9b347002ea1a057d5",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "a580f65cd51448fb9481180c17adc763"
    },
    {
      "cell_type": "code",
      "source": "import tensorflow as tf\nimport seaborn as sns\nimport numpy as np\nfrom PIL import Image\nimport glob\nfrom collections import defaultdict\nfrom tensorflow import keras\nfrom tensorflow.keras import layers",
      "metadata": {
        "cell_id": "4dc0ec223e1e4c30b234a139240a659d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "4dc0ec223e1e4c30b234a139240a659d",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "IMG_SIZE = (94, 125)\n\ndef pixels_from_path(file_path):\n    im = Image.open(file_path)\n\n    im = im.resize(IMG_SIZE)\n    np_im = np.array(im)\n    # Matrix of pixel RGB values\n\n    return np_im\n\nshape_counts = defaultdict(int)\n\nfor i, cat in enumerate(glob.glob('cats/*')[:1000]):\n    if i%100 == 0:\n        print(i)\n    img_shape = pixels_from_path(cat).shape\n    shape_counts[str(img_shape)]= shape_counts[str(img_shape)]+ 1\n\nshape_items = list(shape_counts.items())\nshape_items.sort(key = lambda x: x[1])\nshape_items.reverse()\n\n# 10% of the data will automatically be used for validation\nvalidation_size = 0.1\nimg_size = IMG_SIZE # resize images to be 374x500 (most common shape)\nnum_channels = 3 # RGB\nsample_size = 8192 # We'll use 8192 pictures (2**13)\npixels_from_path(glob.glob('cats/*')[5]).shape\n\nSAMPLE_SIZE = 2048\n\nprint(\"loading training cat images...\")\ncat_train_set = np.asarray([pixels_from_path(cat) for cat in glob.glob('cats/*')[:SAMPLE_SIZE]])\n\nprint(\"loading training dog images...\")\ndog_train_set = np.asarray([pixels_from_path(dog) for dog in glob.glob('dogs/*')[:SAMPLE_SIZE]])\n\nvalid_size = 512\n\nprint(\"loading validation cat images...\")\ncat_valid_set = np.asarray([pixels_from_path(cat) for cat in glob.glob('cats/*')[-valid_size:]])\n\nprint(\"loading validation dog images...\")\ndog_valid_set = np.asarray([pixels_from_path(dog) for dog in glob.glob('dogs/*')[-valid_size:]])\n\nx_train = np.concatenate([cat_train_set, dog_train_set])\nlabels_train = np.asarray([1 for _ in range(SAMPLE_SIZE)]+[0 for _ in range(SAMPLE_SIZE)]) x_valid = np.concatenate([cat_valid_set, dog_valid_set])\nlabels_valid = np.asarray([1 for _ in range(valid_size)]+[0 for _ in range(valid_size)])",
      "metadata": {
        "cell_id": "96a2661ac5ee4e978ad96619855350d4",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "47269047f0224fb6ac29c1082f1b875b",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# CNN Implementation",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "656b8a484ac340c0aa35e260c1925d38",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "5adb8554369146fd8cb5126496f5483e"
    },
    {
      "cell_type": "code",
      "source": "total_pixels = img_size[0] * img_size[1] * 3\nfc_size = 512\ninputs = keras.Input(shape=(img_size[1], img_size[0], 3), name = 'ani_image')\nx = layers.Flatten(name = 'flattened_img')(inputs) # turn image to vector.\nx = layers.Dense(fc_size, activation = 'relu', name = 'first_layer')(x)\noutputs = layers.Dense(1, activation = 'sigmoid', name = 'class')(x)\n\nmodel = keras.Model(inputs = inputs, outputs = outputs)",
      "metadata": {
        "cell_id": "31e64201a6af44c18e77a1a976fecf7d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "bd3354114f204dea82f209bb930ee8f3",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# CNN Training",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "bd2b3e13279140bf939a2956100ba117",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "507e670a408a418b863eb32b815deb74"
    },
    {
      "cell_type": "code",
      "source": "customAdam = keras.optimizers.Adam(lr = 0.001)\nmodel.compile(optimizer = customAdam,  # Optimizer\n              # Loss function to minimize\n              loss = \"mean_squared_error\",\n              # List of metrics to monitor\n              metrics = [\"binary_crossentropy\",\"mean_squared_error\"])\n              \nprint('# Fit model on training data')\n  \nhistory = model.fit(x_train, \n                    labels_train, \n                    batch_size = 32, \n                    shuffle = True, # important since we loaded cats first, dogs second.\n                    epochs = 3,\n                    validation_data = (x_valid, labels_valid))\n                    \n#Train on 4096 samples, validate on 2048 samples\n#loss: 0.5000 - binary_crossentropy: 8.0590 - mean_squared_error: 0.5000 - val_loss: 0.5000 - val_binary_crossentropy: 8.0591 - val_mean_squared_error: 0.5000",
      "metadata": {
        "cell_id": "8c0a6b53429d4627bccb4c5453579b92",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "246f878f2b21418ca18225aa7ac3fb7f",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Train It",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "e2cbd6ab82244639a6482e59efe93b69",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "c9cb8c0212de4c7999653644ebfdfa2f"
    },
    {
      "cell_type": "code",
      "source": "fc_layer_size = 128\nimg_size = IMG_SIZE\n  \nconv_inputs = keras.Input(shape=(img_size[1], img_size[0],3), name='ani_image')\nconv_layer = layers.Conv2D(24, kernel_size=3, activation='relu')(conv_inputs)\nconv_layer = layers.MaxPool2D(pool_size=(2,2))(conv_layer)\nconv_x = layers.Flatten(name = 'flattened_features')(conv_layer) #turn image to vector.\n  \nconv_x = layers.Dense(fc_layer_size, activation='relu', name='first_layer')(conv_x)\nconv_x = layers.Dense(fc_layer_size, activation='relu', name='second_layer')(conv_x)\nconv_outputs = layers.Dense(1, activation='sigmoid', name='class')(conv_x)\n  \nconv_model = keras.Model(inputs=conv_inputs, outputs=conv_outputs)\n  \ncustomAdam = keras.optimizers.Adam(lr=1e-6)\nconv_model.compile(optimizer=customAdam,  # Optimizer\n              # Loss function to minimize\n              loss=\"binary_crossentropy\",\n              # List of metrics to monitor\n              metrics=[\"binary_crossentropy\",\"mean_squared_error\"])\n              \n#Epoch 5/5 loss: 1.6900 val_loss: 2.0413 val_mean_squared_error: 0.3688\nprint('# Fit model on training data')\n  \nhistory = conv_model.fit(x_train, \n                    labels_train, #we pass it th labels\n                    #If the model is taking too long to train, make this bigger\n                    #If it is taking too long to load for the first epoch, make this smaller\n                    batch_size=32, \n                    shuffle = True,\n                    epochs=5,\n                    # We pass it validation data to\n                    # monitor loss and metrics\n                    # at the end of each epoch\n                    validation_data=(x_valid, labels_valid))\n  \npreds = conv_model.predict(x_valid)\npreds = np.asarray([pred[0] for pred in preds])\nnp.corrcoef(preds, labels_valid)[0][1] # 0.15292172",
      "metadata": {
        "cell_id": "fdcd05488ac3459bb5b6049c4a9d03ea",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "29a900eea53f435e91ef6db22785104f",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Analyze",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "33e3c0825ee24acbba93b84c0a0a944d",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "a9fe269a291b4668b96511b69b877830"
    },
    {
      "cell_type": "code",
      "source": "sns.scatterplot(x= preds, y= labels_valid)\n\ncat_quantity = sum(labels_valid)\n  \nfor i in range(1,10):\n    print('threshold :' + str(.1*i))\n    print(sum(labels_valid[preds  .1*i])/labels_valid[preds  .1*i].shape[0])",
      "metadata": {
        "cell_id": "fc5f30fbabc042d1a67539ba658da777",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "3561258e0cc842dc80819f4b14639851",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Scatterplot",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "321cff5b866d47b499bc4a1fdeadf308",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "20aa5d6bb606414094f6c78dec784a6b"
    },
    {
      "cell_type": "code",
      "source": "def animal_pic(index):\n    return Image.fromarray(x_valid[index])\n\ndef cat_index(index):\n    return conv_model.predict(np.asarray([x_valid[124]]))[0][0]",
      "metadata": {
        "cell_id": "383dfc36e8d34359bb2b80e6dca16d0f",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "f1fb235f8b4446f2ad43f74e39b3e036",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Save Model",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "002b388e33f94ab48744e56644c0f496",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "0b94f993c4f349a4b1c878a4070afe4b"
    },
    {
      "cell_type": "code",
      "source": "conv_model.save('conv_model_big')\n\n# An example output would be:\n\nindex = 600\n\nprint(\"probability of being a cat: {}\".format(cat_index(index)))\n\nanimal_pic(index)",
      "metadata": {
        "cell_id": "3d5be0fcb4ea46e0ac3cf675c4a932cd",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "40d98ae909134bb9adcf77642d388de3",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Interface",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "13409c32b7fa4afc9439944bb4a2f8cc",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "48d787b3da964ba3ba5a297b4a7dadc2"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "cell_id": "6a9c2a878a6b4b3a932517c4e5c80d29",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "07be18e153e14c05acb31f8f6544de1a",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3e480598-2102-4b3c-a7af-02aec7eef77c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "02805f786d7a4d39867d9a3c932e8033",
    "deepnote_execution_queue": []
  }
}