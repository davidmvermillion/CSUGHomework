{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# GAN Using Keras and CIFAR10",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "a479523ddd364a0686f16376c836c114",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "4cc7706222ac402b88d43d94990cb19f"
    },
    {
      "cell_type": "markdown",
      "source": "# Import Packages",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "cd3db4fd9d8f413fbd9c0d12ee5dc2e8",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "295784961a2840c7af1df96b8d8d2a90"
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam, SGD",
      "metadata": {
        "source_hash": "93553ae3",
        "execution_start": 1728264417833,
        "execution_millis": 2538,
        "execution_context_id": "14d1314a-a413-4bde-8fea-75dd8b2b0d4d",
        "cell_id": "ec1863628d2d419e9bba3ef5d64011fd",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-10-06 21:26:57.952055: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-10-06 21:26:58.067860: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-10-06 21:26:58.647098: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n2024-10-06 21:26:58.647151: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n2024-10-06 21:26:58.647154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
          "output_type": "stream"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.examples'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtutorials\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmnist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_data\n\u001b[1;32m     11\u001b[0m mnist \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mread_data_sets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# defining functions for the two networks.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Both the networks have two hidden layers\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Generator network function\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples'"
          ]
        }
      ],
      "outputs_reference": null,
      "execution_count": 1,
      "block_group": "ec1863628d2d419e9bba3ef5d64011fd",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Load Data",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "d1e29342cff2454983dc6f129801b088",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "3105c027965f488cb313829d88e9fd12"
    },
    {
      "cell_type": "code",
      "source": "(X, y), (_, _) = keras.datasets.cifar10.load_data()\n\n# Selecting a single class of images\n# The number was randomly chosen and any number\n# between 1 and 10 can be chosen\nX = X[y.flatten() == 8]",
      "metadata": {
        "cell_id": "edbaf11f9c8648079d2b9ed4a9747ef4",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "2e7fe5e42bbb4e558fe11211aefe3e98",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Define Parameters",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "6e0eedc9ef15450dba5b026989e614ae",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "4762ff74c8874a478f87a048945bdfab"
    },
    {
      "cell_type": "code",
      "source": "# Defining the Input shape\nimage_shape = (32, 32, 3)\nlatent_dimensions = 100",
      "metadata": {
        "cell_id": "18a28d67607741d98d7645507dd90331",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "1fc7ca16160846469136a607bef68541",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Generator Function",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "884b3a277a324f4f8428d3835adc1bf9",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "5e2ac576021944ad82ae04ad3238204d"
    },
    {
      "cell_type": "code",
      "source": "def build_generator():\n\n        model = Sequential()\n\n        # Building the input layer\n        model.add(Dense(128 * 8 * 8, activation = \"relu\",\n                        input_dim = latent_dimensions))\n        model.add(Reshape((8, 8, 128)))\n        model.add(UpSampling2D())        \n        model.add(Conv2D(128, kernel_size = 3, padding = \"same\"))\n        model.add(BatchNormalization(momentum = 0.78))\n        model.add(Activation(\"relu\"))\n        model.add(UpSampling2D())\n        model.add(Conv2D(64, kernel_size = 3, padding = \"same\"))\n        model.add(BatchNormalization(momentum = 0.78))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(3, kernel_size = 3, padding = \"same\"))\n        model.add(Activation(\"tanh\"))\n\n        # Generating the output image\n        noise = Input(shape = (latent_dimensions,))\n        image = model(noise)\n\n        return Model(noise, image)",
      "metadata": {
        "cell_id": "3f2eec849ab04ef49bd13d573c3579eb",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "8053f70b007745ff9ccf923e3ad787d9",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Discriminator Function",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "aeacebfe3329451891afde85007c32cb",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "d05f5e03302d4d2697b35b46d97b222c"
    },
    {
      "cell_type": "code",
      "source": "def build_discriminator(): \n\n        # Building the convolutional layers\n        # to classify whether an image is real or fake\n        model = Sequential()\n        model.add(Conv2D(32, kernel_size = 3, strides = 2,\n                         input_shape = image_shape, padding = \"same\"))\n        model.add(LeakyReLU(alpha = 0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(64, kernel_size = 3, strides = 2, padding = \"same\"))\n        model.add(ZeroPadding2D(padding = ((0, 1),(0, 1))))\n        model.add(BatchNormalization(momentum = 0.82))\n        model.add(LeakyReLU(alpha = 0.25))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, kernel_size = 3, strides = 2, padding = \"same\"))\n        model.add(BatchNormalization(momentum = 0.82))\n        model.add(LeakyReLU(alpha = 0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(256, kernel_size 3 , strides = 1, padding = \"same\"))\n        model.add(BatchNormalization(momentum = 0.8))\n        model.add(LeakyReLU(alpha = 0.25))\n        model.add(Dropout(0.25))\n\n        # Building the output layer\n        model.add(Flatten())\n        model.add(Dense(1, activation = 'sigmoid'))\n        image = Input(shape = image_shape)\n        validity = model(image)\n\n        return Model(image, validity)",
      "metadata": {
        "cell_id": "5c4eb3931ac24a2fab2de40ccfba16df",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "5136da028331493896a80de42a8a7b51",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Image Generator",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "41637b3d080e460a957e550c2ba38e2b",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "0e87885b581f41ea9a18a559b7559245"
    },
    {
      "cell_type": "code",
      "source": "def display_images():\n\n        r, c = 4, 4\n        noise = np.random.normal(0, 1, (r * c, latent_dimensions))\n        generated_images = generator.predict(noise)\n\n        # Scaling the generated images\n        generated_images = 0.5 * generated_images + 0.5\n        fig, axs = plt.subplots(r, c)\n        count = 0\n\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(generated_images[count, :, :,])\n                axs[i,j].axis('off')\n                count += 1\n\n        plt.show()\n        plt.close()",
      "metadata": {
        "cell_id": "43f23304c6d5439e82ee9c80057eba6a",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "5c339c253aee45a6b44f6574fd8e3586",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Build GAN",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "6f0e7290657448f4a7d3ea518d27958e",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "466333ea9b464e61952b4999b454dae5"
    },
    {
      "cell_type": "code",
      "source": "# Building and compiling the discriminator\ndiscriminator = build_discriminator()\n\ndiscriminator.compile(\n    loss = 'binary_crossentropy', \n    optimizer = Adam(0.0002, 0.5), \n    metrics = ['accuracy']\n    ) \n\n# Making the discriminator untrainable\n# so that the generator can learn from fixed gradient\ndiscriminator.trainable = False\n\n# Building the generator\ngenerator = build_generator()\n\n# Defining the input for the generator and generating the images\nz = Input(shape = (latent_dimensions,))\nimage = generator(z)\n\n# Checking the validity of the generated image\nvalid = discriminator(image)\n\n# Defining the combined model of the generator and the discriminator\ncombined_network = Model(z, valid)\ncombined_network.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(0.0002, 0.5)\n    )",
      "metadata": {
        "cell_id": "a32c28713a004bd3b125f8e6cb7c79c3",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "466281e66e3c422a822bb7c46b370532",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "# Train Network",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ac98100198814b9f870bbb0fb35ee04d",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "8e8d521e3da44c708d8662a75c3fe63c"
    },
    {
      "cell_type": "code",
      "source": "num_epochs = 15000\nbatch_size = 32\ndisplay_interval. = 2500\nlosses. =[]\n\n# Normalizing the input\nX = (X / 127.5) - 1.\n\n# Defining the Adversarial ground truths\nvalid = np.ones((batch_size, 1))\n\n# Adding some noise \nvalid += 0.05 * np.random.random(valid.shape)\nfake = np.zeros((batch_size, 1))\nfake += 0.05 * np.random.random(fake.shape)\n\nfor epoch in range(num_epochs):\n\n            # Training the Discriminator\n\n            # Sampling a random half of images\n            index = np.random.randint(0, X.shape[0], batch_size)\n            images = X[index]\n\n            # Sampling noise and generating a batch of new images\n            noise = np.random.normal(0, 1, (batch_size, latent_dimensions))\n            generated_images = generator.predict(noise)\n\n            # Training the discriminator to detect more accurately\n            # whether a generated image is real or fake\n            discm_loss_real = discriminator.train_on_batch(images, valid)\n            discm_loss_fake = discriminator.train_on_batch(generated_images, fake)\n            discm_loss = 0.5 * np.add(discm_loss_real, discm_loss_fake)\n\n            # Training the generator\n\n            # Training the generator to generate images\n            # that pass the authenticity test\n            genr_loss = combined_network.train_on_batch(noise, valid)\n\n            # Tracking training progress                \n            if epoch % display_interval == 0:\n                 display_images()",
      "metadata": {
        "cell_id": "f8140101319c483d8675874473a381f9",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": null,
      "block_group": "1a48123cdbab4b7fa39a390ca44c9353",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=104e7b74-4307-4537-9235-5299b504c876' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "3411272515ef4a24b8ced548f31af123",
    "deepnote_execution_queue": []
  }
}