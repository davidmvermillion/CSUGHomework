{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/0.16/modules/generated/sklearn.datasets.make_moons.html\n",
    "# https://github.com/nageshsinghc4/Artificial-Neural-Network-from-scratch-python/blob/master/ANN_with_one_hidden_layer.py\n",
    "# https://www.theaidream.com/post/build-an-artificial-neural-network-ann-from-scratch\n",
    "# https://towardsdatascience.com/building-an-artificial-neural-network-using-pure-numpy-3fe21acc5815\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup\n",
    "def generate_data(points, noisiness):\n",
    "    np.random.seed(2024)\n",
    "    X, y = datasets.make_moons(points, noise = noisiness)\n",
    "    return X, y\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
    "        return np.maximum(0,input)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
